<p align="center">
  <img src="assets/logo/sentinellogo.png" alt="Sentinel Logo" width="200">
</p>

# Sentinel prevents LLM regressions from reaching production.

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![SDK: v0.5.0](https://img.shields.io/badge/SDK-v0.5.0-green.svg)](#)

---

## The Problem

LLM outputs change unexpectedly. Same prompt, different model version â†’ different behavior.  
Without Sentinel, you discover this **in production**.

## The Solution

```python
from sdk.decorator import trace, expect
from sdk.context import execution

@trace(provider="gemini")
@expect(must_include=["refund"], max_latency_ms=1500)
def customer_reply(query):
    return llm.generate(query)

# Track multi-step agent flows
with execution("customer-support-flow"):
    result = customer_reply("I want a refund")
```

```bash
# Mark a known-good response as baseline
sentinel bless <trace_id>

# In CI: fail if output regresses
sentinel check  # exits 1 on failure

# Analyze execution graphs
sentinel graph-check <execution_id>
```

That's it. Your CI now blocks LLM regressions.

---

## Quick Start

```bash
git clone https://github.com/xXMohitXx/Sentinel.git
cd Sentinel
pip install -r requirements.txt
python -m cli.main server
```

Open http://127.0.0.1:8000/ui

---

## Features

| Feature | Description |
|---------|-------------|
| **Trace Capture** | Record every LLM call automatically |
| **Expectations** | Define PASS/FAIL rules (4 deterministic rules) |
| **Golden Traces** | Baseline comparisons with hash verification |
| **CI Integration** | `sentinel check` exits 1 on regression |
| **Failure-First UI** | See what broke in < 10 seconds |
| **Execution Graphs** | Visualize multi-step agent workflows |
| **Forensics Mode** | Debug failures with guided investigation |
| **Enterprise Hardening** | Integrity hashing, snapshots, exports |

---

## Commands

| Command | What it does |
|---------|--------------|
| `sentinel init` | Initialize config |
| `sentinel server` | Start API server |
| `sentinel list` | List traces |
| `sentinel list --failed` | Show only failed traces |
| `sentinel show <id>` | Show trace details |
| `sentinel replay <id>` | Re-run a trace |
| `sentinel bless <id>` | Mark as golden baseline |
| `sentinel check` | CI regression check |
| `sentinel graph-check <id>` | CI graph verdict check |

---

## Execution Graphs (New!)

Track multi-step LLM workflows with causality:

```python
from sdk.context import execution

with execution("my-agent"):
    step1 = call_llm("First step")
    step2 = call_llm("Second step")  # Tracks parent-child
```

**Graph Features:**
- ğŸ“Š DAG visualization with stages
- ğŸ”¬ Forensics mode for debugging
- â±ï¸ Time-scaled nodes by latency
- ğŸ” Investigation paths
- ğŸ”’ Enterprise: integrity hashing, exports

---

## Expectations

```python
@expect(
    must_include=["word"],       # Required content
    must_not_include=["sorry"],  # Forbidden content
    max_latency_ms=2000,         # Performance gate
    min_tokens=10                # Minimum length
)
```

All rules are deterministic. No AI judgment. No ambiguity.

---

## CI Integration

```yaml
# .github/workflows/sentinel.yml
- run: python -m cli.main check
  env:
    GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
```

Pipeline fails if any golden trace regresses.

---

## When NOT to Use Sentinel

- You're experimenting casually
- You don't care about regressions
- You're building a prototype
- You don't have LLM calls in production

Sentinel is infrastructure, not a toy.

---

## Architecture

```
sentinel/
â”œâ”€â”€ sdk/                  # Python SDK (v0.5.0)
â”‚   â”œâ”€â”€ schema.py         # Trace schema
â”‚   â”œâ”€â”€ decorator.py      # @trace, @expect
â”‚   â”œâ”€â”€ context.py        # Execution context
â”‚   â”œâ”€â”€ graph.py          # Graph models (Phase 14+)
â”‚   â”œâ”€â”€ expectations/     # 4 rules + evaluator
â”‚   â””â”€â”€ adapters/         # OpenAI, Gemini
â”œâ”€â”€ server/               # FastAPI backend
â”œâ”€â”€ cli/                  # Command-line interface
â”œâ”€â”€ ui/                   # Failure-first web UI
â””â”€â”€ tests/                # Unit tests
```

---

## Status: âœ… COMPLETE (v0.5.0)

All 25 phases implemented:

- âœ… SDK with OpenAI & Gemini adapters
- âœ… FastAPI server with REST API
- âœ… CLI with all commands
- âœ… Expectation Engine (4 deterministic rules)
- âœ… Golden Traces with hash comparison
- âœ… CI integration (`sentinel check`)
- âœ… Failure-First UI
- âœ… Execution Graphs & DAG visualization
- âœ… Semantic Nodes & Hierarchical Stages
- âœ… Forensics Mode & Investigation Paths
- âœ… Enterprise Hardening (integrity, export)

---

## Contributing

See [DEVELOPMENT.md](DEVELOPMENT.md) for setup and contribution guidelines.

---

## Links

- [Documentation](DOCUMENTATION.md)
- [Development Guide](DEVELOPMENT.md)
- [GitHub](https://github.com/xXMohitXx/Sentinel)

---

MIT License
